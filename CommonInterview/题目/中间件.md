1. RPC和MQ的区别
   RPC（Remote Procedure Call）远程过程调用，主要解决远程通信间的问题，不需要了解底层网络的通信机制。
   RPC的一般需要经历4个步骤：
   1、建立通信
   2、服务寻址
   3、网络传输  1）序列化  2）反序列化
   4、服务调用
   
   消息队列（MQ）是一种能实现生产者到消费者单向通信的通信模型，一般来说是指实现这个模型的中间件。
   典型的特点：
   1、解耦   
   2、可靠投递  
   3、广播   
   4、最终一致性 
   5、流量削峰 
   6、消息投递保证 
   7、异步通信（支持同步）
   8、提高系统吞吐、健壮性
   
   区别：
   1. 在架构上，RPC和MQ的差异点是，Message有一个中间结点Message Queue，可以把消息存储。
   2. 同步调用：对于要立即等待返回处理结果的场景，RPC是首选。  
   3. MQ 的使用，一方面是基于性能的考虑，比如服务端不能快速的响应客户端（或客户端也不要求实时响应），需要在队列里缓存。
      另外一方面，它更侧重数据的传输，因此方式更加多样化，除了点对点外，还有订阅发布等功能。
   4. 而且随着业务增长，有的处理端处理量会成为瓶颈，会进行同步调用改造为异步调用，这个时候可以考虑使用MQ。
2. kafka重复消息，丢失消息
3. nginx原理，几个进程/，master进程作用/，配置更新后，worker线程如何知道更新/，master挂了怎么办，成了孤儿进程/，来了请求，如何知道那个worker处理/,nginx惊群
   反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。
   反向代理服务器对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理的命名空间(name-space)中的内容发送普通请求，接着反向代理服务器将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容原本就是它自己的一样。
   
   nginx 采用的是多进程（单线程） + io多路复用(epoll)模型 实现高并发
   
   解析初始化配置文件后会 创建（fork）一个master进程 之后 这个进程会退出 
   master 进程会 变为孤儿进程 由init进程托管。（可以通过python 或php 启动后创建子进程,然后杀死父进程得见子进程会由init进程托管）
   
   Nginx的负载均衡模块采用两种方法：
   
   轮转法，它处理请求就像纸牌游戏一样从头到尾分发；
   
   IP哈希法，在众多请求的情况下，它确保来自同一个IP的请求会分发到相同的后端服务器。
   
   
   多进程处理模型：
   首先，master进程一开始就会根据我们的配置，来建立需要listen的网络socket fd，然后fork出多个worker进程。
   
   其次，根据进程的特性，新建立的worker进程，也会和master进程一样，具有相同的设置。因此，其也会去监听相同ip端口的套接字socket fd。
   
   然后，这个时候有多个worker进程都在监听同样设置的socket fd，意味着当有一个请求进来的时候，所有的worker都会感知到。这样就会产生所谓的“惊群现象”。为了保证只会有一个进程成功注册到listenfd的读事件，nginx中实现了一个“accept_mutex”类似互斥锁，只有获取到这个锁的进程，才可以去注册读事件。其他进程全部accept 失败。
   
   最后，监听成功的worker进程，读取请求，解析处理，响应数据返回给客户端，断开连接，结束。因此，一个request请求，只需要worker进程就可以完成。
   
   工作流程：
   1. 用户通过域名发出访问Web服务器的请求，该域名被DNS服务器解析为反向代理服务器的IP地址；
   2. 反向代理服务器接受用户的请求；
   3. 反向代理服务器在本地缓存中查找请求的内容，找到后直接把内容发送给用户； 
   4. 如果本地缓存里没有用户所请求的信息内容，反向代理服务器会代替用户向源服务器请求同样的信息内容，并把信息内容发给用户，如果信息内容是缓存的还会把它保存到缓存中
   
   优点：
   1. 保护了真实的web服务器，保证了web服务器的资源安全
   2. 节约了有限的IP地址资源
   3. 减少WEB服务器压力，提高响应速度
      反向代理就是通常所说的web服务器加速，它是一种通过在繁忙的web服务器和外部网络之间增加一个高速的web缓冲服务器来降低实际的web服务器的负载的一种技术。
   4. 其他优点
     （1）请求的统一控制，包括设置权限、过滤规则等；
     （2）区分动态和静态可缓存内容；
     （3）实现负载均衡，内部可以采用多台服务器来组成服务器集群，外部还是可以采用一个地址访问；
     （4）解决Ajax跨域问题；
     （5）作为真实服务器的缓冲，解决瞬间负载量大的问题；   
      
   Nginx有五大优点：模块化、事件驱动、异步、非阻塞、多进程单线程。  由内核和模块组成的，其中内核完成的工作比较简单，仅仅通过查找配置文件将客户端请求映射到一个location block，然后又将这个location block中所配置的每个指令将会启动不同的模块去完成相应的工作。
   
   客户端发送HTTP请求 –>
   
   Nginx基于配置文件中的位置选择一个合适的处理模块 ->
   
   (如果有)负载均衡模块选择一台后端服务器 –>
   
   处理模块进行处理并把输出缓冲放到第一个过滤模块上 –>
   
   第一个过滤模块处理后输出给第二个过滤模块 –>
   
   然后第二个过滤模块又到第三个 –>
   
   依此类推 –> 最后把响应发给客户端。
   
   Nginx在启动时会以daemon形式在后台运行，采用多进程+异步非阻塞IO事件模型来处理各种连接请求。多进程模型包括一个master进程，多个worker进程，一般worker进程个数是根据服务器CPU核数来决定的。master进程负责管理Nginx本身和其他worker进程。
   Master进程负责管理Nginx本身和其他worker进程，worker进程都是从父进程fork出来的，并且父进程的ppid为1，表示其为daemon进程。在nginx多进程中，每个worker都是平等的，因此每个进程处理外部请求的机会权重都是一致的。
   
   Master进程的作用是？
   读取并验证配置文件nginx.conf；管理worker进程；
 
   Worker进程的作用是？
   每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 

   Nginx如何做到热部署？
   修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。

   Nginx挂了怎么办？
   Nginx既然作为入口网关，很重要，如果出现单点问题，显然是不可接受的。Keepalived+Nginx实现高可用。
   Keepalived+Nginx实现高可用的思路：
   第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP）
   第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,，从而实现Nginx故障切换）
   
   master 进程挂了worker会 变为孤儿进程 由init（ppid 为1 ）进程托管。 
   
   Nginx如何做到高并发下的高效处理？
   Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可。
   
   1. master 
      首先nginx 创建一个master 进程，通过socket() 创建一个sock文件描述符用来监听（sockfd）
      绑定端口(bind) 开启监听（listen）。
      nginx 一般监听80（http） 或 443 （https）端口
     （fork 多个子进程后，master 会监听worker进程，和等待信号）
   
   2. worker
      然后 创建(fork)多个 worker子进程（复制master 进程的数据），
      此时所有的worker进程 继承了sockfd(socket文件描述符)，
      当有连接进来之后 worker进程就可以accpet()创建已连接描述符，
      然后通过已连接描述符与客户端通讯
      
   worker 进程中，ngx_worker_process_cycle()函数就是这个无限循环的处理函数。在这个函数中，一个请求的简单处理流程如下：
   1. 操作系统提供的机制（例如 epoll, kqueue 等）产生相关的事件。
   2. 接收和处理这些事件，如是接收到数据，则产生更高层的 request 对象。
   3. 处理 request 的 header 和 body。
   4. 产生响应，并发送回客户端。
   5. 完成 request 的处理。
   6. 重新初始化定时器及其他事件。   
      
   惊群现象:
      由于worker进程 继承了master进程的sockfd,当连接进来是，所有的子进程都将收到通知并“争着”与
      它建立连接，这就叫惊群现象。大量的进程被激活又挂起，最后只有一个进程accpet() 到这个连接，这会消耗系统资源
     （等待通知，进程被内核全部唤醒，只有一个进程accept成功，其他进程又休眠。这种浪费现象叫惊群）
   
   nginx 对惊群现象的处理:
   原因：
       多个进程监听同一个端口引发的。
   解决：
       如果可以同一时刻只能有一个进程监听端口，这样就不会发生“惊群”了，此时新连接事件只能唤醒正在监听的唯一进程。
       如何保持一个时刻只能有一个worker进程监听端口呢？nginx设置了一个accept_mutex锁，在使用accept_mutex锁时，
       只有进程成功调用了ngx_trylock_accept_mutex方法获取锁后才可以监听端口
       
   worker进程会竞争监听客户端的连接请求：这种方式可能会带来一个问题，就是可能所有的请求都被一个worker进程给竞争获取了，导致其他进程都比较空闲，而某一个进程会处于忙碌的状态，这种状态可能还会导致无法及时响应连接而丢弃discard掉本有能力处理的请求。这种不公平的现象，是需要避免的，尤其是在高可靠web服务器环境下。
   
   针对这种现象，Nginx采用了一个是否打开accept_mutex选项的值，ngx_accept_disabled标识控制一个worker进程是否需要去竞争获取accept_mutex选项，进而获取accept事件。
   
   ngx_accept_disabled值：nginx单进程的所有连接总数的八分之一，减去剩下的空闲连接数量，得到的这个ngx_accept_disabled。
   
   当ngx_accept_disabled大于0时，不会去尝试获取accept_mutex锁，并且将ngx_accept_disabled减1，于是，每次执行到此处时，都会去减1，直到小于0。不去获取accept_mutex锁，就是等于让出获取连接的机会，很显然可以看出，当空闲连接越少时，ngx_accept_disable越大，于是让出的机会就越多，这样其它进程获取锁的机会也就越大。不去accept，自己的连接就控制下来了，其它进程的连接池就会得到利用，这样，nginx就控制了多进程间连接的平衡了。    
       
   worker工作：
   从上图中，我们可以看到worker进程做了
   1、accept() 与客户端建立连接
   2、recv()接收客户端发过来的数据
   3、send() 向客户端发送数据
   4、close() 关闭客户端连接    
     
4. activemq延时队列，广播，prefetch机制
   
   1. 第一步需要修改activemq.xml配置文件，开启延时发送 schedulerSupport="true"
   2. 第二步消息生产者在发送消息的时候需进行设置
   
   为了高效使用网络资源，broker使用push模型把消息分发给消费者。这样可以确保消费者的本地消息缓冲区中，总有待处理的消息。这就是activeMq的prefetch机制，相较于消费者显式的从broker中pull消息。预先将消息push到每个消费者的缓冲区可以降低消息的处理时延.
   每次consumer连接至 broker 时，broker 预先获取许多message到消费者（前提是broker 中存在大量消息），预先存放message的数量取决于prefetchSize（默认为1000）。
   此机制的目的很显然，是想让客户端代码用一个consumer反复进行receive操作，这样能够大量提高出队性能。
   
   prefetch预取，即push，而不用pull，pull拉会造成消息一定程度的延迟；
 
5. 消息队列QOS
   RabbitMQ可以设置basicQoS(Consumer Prefetch)来对consumer进行流控，从而限制未ack的消息数量。
   维护了一个阻塞队列BlockingQueue，此队列就是用来缓存从queue获取的message。
6. kafka底层算法，kafka存储机制，为什么这么快
   1. topic中partition存储分布
   2. partiton中文件存储方式
   3. partiton中segment文件存储结构
      在每个partition中有可以分为多个segment file。当生产者往partition中存储数据时，内存中存不下了，就会往segment file里面存储。kafka默认每个segment file的大小是500M，在存储数据时，会先生成一个segment file，当这个segment file到500M之后，再生成第二个segment file 以此类推。每个segment file对应两个文件，分别是以.log结尾的数据文件和以.index结尾的索引文件。在服务器上，每个partition是一个文件夹，每个segment是一个文件。
      每个segment file也有自己的命名规则，每个名字有20个字符，不够用0填充。每个名字从0开始命名，下一个segment file文件的名字就是，上一个segment file中最后一条消息的索引值。在.index文件中，存储的是key-value格式的，key代表在.log中按顺序开始第条消息，value代表该消息的位置偏移。但是在.index中不是对每条消息都做记录，它是每隔一些消息记录一次，避免占用太多内存。即使消息不在index记录中，在已有的记录中查找，范围也大大缩小了。
   4. 在partition中怎样通过offset查找message
7. git的cherry-pick命令
   把分支A的部分提交拷到分支B中
   使用方法：
   
   1、切到分支B
   
   2、假设分支A上的提交id分别是aaa,bbb,ccc,ddd，直接输入命令：git cherry-pick aaa^..ddd (^表示包括左边的，即[aaa,ddd]，不加则表示不包括左边的，即(aaa,ddd])
   
   3、回车之后，看到哪个提交上挂了，假设aaa的时候就GG了，就先解决冲突，而后 add .、commit -m
   
   4、取消之前的cherry-pick：git cherry-pick --quit
   
   5、再cherry-pick：git cherry-pick aaa..ddd
   
   6、如果出现 bad version，就重复步骤5，只不过每次减少一次提交（依次去掉ddd，ccc等），直到成功
   
   7、而后把剩下的提交依次cherry-pick上就行，一般是有冲突，有冲突的话，解决就中了
8. es分片，增加节点后的再平衡过程
9. 常见消息队列特征，桥接，延时队列，kafka扩容
10. es和solr区别
11. es动态scheme相关问题
12. 如何在kafka总标识唯一请求，kafka事务机制
13. nginx worker如何使用新配置？配置地区通过共享内存，采用copy更新策略
14. es一致性实现
    一个Elasticsearch集群(下面简称ES集群)是由许多节点(Node)构成的，Node可以有不同的类型，通过以下配置，可以产生四种不同类型的Node：
    四种不同类型的Node是一个node.master和node.data的true/false的两两组合。
    当node.master为true时，其表示这个node是一个master的候选节点，可以参与选举，在ES的文档中常被称作master-eligible node，类似于MasterCandidate。ES正常运行时只能有一个master(即leader)，多于1个时会发生脑裂。
    当node.data为true时，这个节点作为一个数据节点，会存储分配在该node上的shard的数据并负责这些shard的写入、查询等。
    
    写一致性
    对于写操作，Elasticsearch支持与大多数其他数据库不同的一致性级别，它允许初步检查以查看有多少个分片可用于允许写入。可用的选项是：quorum的可设置的值为：one和all。默认情况下，它被设置为：quorum，这意味着只有当大多数分片可用时，才允许写入操作。在大部分分片可用的情况下，由于某种原因，写入复制副本分片失败仍然可能发生，在这种情况下，副本被认为是错误的，该分片将在不同的节点上进行重建。 
    
    我们在发送任何一个增删改操作的时候，比如说put /index/type/id，都可以带上一个consistency参数，指明我们想要的写一致性是什么？
    put /index/type/id?consistency=quorum
     
    one：要求我们这个写操作，只要有一个primary shard是active活跃可用的，就可以执行
    all：要求我们这个写操作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作
    quorum：默认的值，要求所有的shard中，必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作
    quorum机制
    写之前必须确保大多数shard都可用，int( (primary + number_of_replicas) / 2 ) + 1，当number_of_replicas>1时才生效，number_of_replicas是在索引中的的设置，用来定义复制分片的数量，如果只有一台机器（没有备机），那么就为0。

    读一致性
    对于读操作，新文档在刷新间隔时间后才能用于搜索。为了确保搜索结果来自最新版本的文档，可以将复制(replication)设置为sync（默认值）（同步），当在主分片和副本分片的写操作都完成后，写操作才返回。在这种情况下，来自任何分片的搜索请求将从文档的最新版本返回结果。甚至如果你的应用要求高索引吞吐率（higher indexing rate）时，replication=async（异步），可以为搜索请求设置 '_preference' 参数为 primary 。这样搜索请求会查询主分片，从而保证结果中的文档是最新版本。
    
    当write consistency不是all的时候，需要指定从primary shard读
    
    当write consistency为all的时候，而且replication是sync模式(默认)，无需额外指定，如果replication是async模式，则需要从primary shard读取。
15. 消息队列的选型
    1. RabbitMQ
       结合erlang语言本身的并发优势，支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正是如此，使的它变的非常重量级，更适合于企业级的开发。
       性能较好，但是不利于做二次开发和维护。
    2. ActiveMQ
       历史悠久的开源项目，是Apache下的一个子项目。已经在很多产品中得到应用，实现了JMS1.1规范，可以和spring-jms轻松融合，实现了多种协议，不够轻巧（源代码比RocketMQ多），支持持久化到数据库，对队列数较多的情况支持不好。

------

16. 为什么kafka不支持读写分离
    1. 数据一致性问题
       复制过程会导致副本数据不一致
    2. 延时问题
17. kafka如何将网络通信性能提升的
    1. 多个partition
    2. batch机制：在客户端放内存缓冲区，消息先写入这里面，然后吧多个消息打包成一个batch,默认16kb，然后一个batch数据通过网络通信。
    3. request机制：多个topic的partition在同一个broker上，因为都是发往同一个broker，可以把多个batch打包成同一个request一次性发送到同一个broker
18. kafka的选举
    1. 控制器的选举
       集群中多个broker,其中一个broker被选举为控制器，负责管理整个集群的所有分区和副本的工作状态等。
       
       比如当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。再比如当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。
       
       Kafka Controller的选举是依赖Zookeeper来实现的，在Kafka集群中哪个broker能够成功创建/controller这个临时（EPHEMERAL）节点他就可以成为Kafka Controller。
    2. 分区leader的选举
       
       分区leader副本的选举由Kafka Controller 负责具体实施。
       基本思路是按照AR集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。
    3. 消费者相关的选举  
       组协调器GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，这个选举的算法也很简单，分两种情况分析。
       
       如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader。
       
       如果某一时刻leader消费者由于某些原因退出了消费组，那么会重新选举一个新的leader，这个重新选举leader的过程又更“随意”了。
       在GroupCoordinator中消费者的信息是以HashMap的形式存储的，其中key为消费者的member_id，而value是消费者相关的元数据信息。
       
       leaderId表示leader消费者的member_id，它的取值为HashMap中的第一个键值对的key，这种选举的方式基本上和随机无异。
       
       总体上来说，消费组的leader选举过程是很随意的。

19. kafka消息的路由
    producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为：
    1. 指定了 patition，则直接使用；
    2. 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition
    3. patition 和 key 都未指定，使用轮询选出一个 patition。
20. kafka消息的存储
    顺序存储，无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：     
    1. 基于时间：log.retention.hours=168
    2. 基于大小：log.retention.bytes=1073741824
21. kafka的topic创建   
    1. controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。
    2. controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition：
    	2.1 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR
    	2.2 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state
    3. controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。
22. 删除 topic
    1. controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。
    2. 若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。    
23. kafak的分区分配
    
    1. 生产者的分区分配
       消息在发往broker之前是需要确定它所发往的分区的，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号。
       
       如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。分区器的作用就是为消息分配分区。
       
       默认情况下，如果消息的key不为null，那么默认的分区器会对key进行哈希（采用MurmurHash2算法，具备高运算性能及低碰撞率）
       
       最终根据得到的哈希值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。
    2. 消费者的分区分配
       在Kafka的默认规则中，每一个分区只能被同一个消费组中的一个消费者消费。消费者的分区分配是指为消费组中的消费者分配所订阅主题中的分区。
       对于消费者的分区分配而言，Kafka自身提供了三种策略，分别为RangeAssignor、RoundRobinAssignor以及StickyAssignor
       
       其中RangeAssignor为默认的分区分配策略
       1. topic下的所有有效分区平铺，例如P0, P1, P2, P3… …
       
       2. 消费者按照字典排序，例如C0, C1, C2
       
       3. 分区数除以消费者数，得到n
       
       4. 分区数对消费者数取余，得到m
       
       5. 消费者集合中，前m个消费者能够分配到n+1个分区，而剩余的消费者只能分配到n个分区。
       
       roundrobin策略
       1. 消费者按照字典排序，例如C0, C1, C2… …，并构造环形迭代器。
       
       2. topic名称按照字典排序，并得到每个topic的所有分区，从而得到所有分区集合。
       
       3. 遍历第2步所有分区集合，同时轮询消费者。
       
       如果轮询到的消费者订阅的topic不包括当前遍历的分区所属topic，则跳过；否则分配给当前消费者，并继续第3步。
       
       sticky策略，sticky中文意思是粘的，粘性的。在kafka这里，是为了保证再分配的结果尽可能多的保有现有分配。
       1. 尽可能保证分配均衡；
       
       2. 当重新分配时，保留尽可能多的现有分配（因为分区从一个消费者转移到另一个消费者是需要一些开销的，尽可能保证现有分配有助于减少这种开销）；
       
       3. 第一个目标的优先级高于第二个目标；
       
       自定义(随机)策略
       自定义实现非常简单，自定义类AfeiAssignor实现抽象类AbstractPartitionAssignor即可
            
    3. broker端的分区分配        
       
       集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。
       
       分区分配是否均衡会影响到Kafka整体的负载均衡，具体还会牵涉到优先副本等概念。
       
       在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；
       
       如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。
       
       使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。
       
       如果集群中所有的broker节点都没有配置broker.rack参数，或者使用disable-rack-aware参数来创建主题，那么采用的就是未指定机架信息的分配策略，否则采用的就是指定机架信息的分配策略。
       
24. Kafka如何通过精妙的架构设计优化JVM GC问题？       
    
    一个Batch中的数据，会取出来然后封装在底层的网络包里，通过网络发送出去到达Kafka服务器。那么然后呢？这个Batch里的数据都发送过去了，现在Batch里的数据应该怎么处理？
    
    这些Batch里的数据此时可还在客户端的JVM的内存里啊！那么此时从代码实现层面，一定会尝试避免任何变量去引用这些Batch对应的数据，然后尝试触发JVM自动回收掉这些内存垃圾。这样不断的让JVM回收垃圾，就可以不断的清理掉已经发送成功的Batch了，然后就可以不断的腾出来新的内存空间让后面新的数据来使用。
    
    JVM GC在回收内存垃圾的时候，他会有一个“Stop the World”的过程，也就是垃圾回收线程运行的时候，会导致其他工作线程短暂的停顿，这样可以便于他自己安安静静的回收内存垃圾。
    
    现在JVM GC是越来越先进，从CMS垃圾回收器到G1垃圾回收器，核心的目标之一就是不断的缩减垃圾回收的时候，导致其他工作线程停顿的时间， 所以现在越是新款的垃圾回收器导致工作线程停顿的时间越短
    
    在Kafka客户端内部，对这个问题实现了一个非常优秀的机制，就是缓冲池的机制
    
    就是每个Batch底层都对应一块内存空间，这个内存空间就是专门用来存放写入进去的消息的。
    然后呢，当一个Batch被发送到了kafka服务器，这个Batch的数据不再需要了，就意味着这个Batch的内存空间不再使用了。
    此时这个Batch底层的内存空间不要交给JVM去垃圾回收，而是把这块内存空间给放入一个缓冲池里。
    这个缓冲池里放了很多块内存空间，下次如果你又有一个新的Batch了，那么不就可以直接从这个缓冲池里获取一块内存空间就ok了？
    
    正是这个设计思想让Kafka客户端的性能和吞吐量都非常的高，这里蕴含了大量的优秀的机制。
    如果我现在把一个缓冲池里的内存资源都占满了，现在缓冲池里暂时没有内存块了，怎么办呢？
    很简单，阻塞你的写入操作，不让你继续写入消息了。把你给阻塞住，不停的等待，直到有内存块释放出来，然后再继续让你写入消息。
    
25. CopyOnWrite及在kafka中的使用
    CopyOnWrite不用加什么读写锁，锁统统给我去掉，有锁就有问题，有锁就有互斥，有锁就可能导致性能低下，你阻塞我的请求，导致我的请求都卡着不能执行。
    写数据的时候利用拷贝的副本来执行
    那那个写线程现在把副本数组给修改完了，现在怎么才能让读线程感知到这个变化呢？
    可以用volatile写的方式，把这个副本数组赋值给volatile修饰的那个数组的引用变量了。
    当然不能多个线程同时更新了，这个时候就是看上面源码里，加入了lock锁的机制，也就是同一时间只有一个线程可以更新。
    
    在Kafka的内核源码中，有这么一个场景，客户端在向Kafka写数据的时候，会把消息先写入客户端本地的内存缓冲，然后在内存缓冲里形成一个Batch之后再一次性发送到Kafka服务器上去，这样有助于提升吞吐量。
    Kafka的内存缓冲数据结构就是核心的用来存放写入内存缓冲中的消息的数据结构,自己实现了一个CopyOnWriteMap，这个CopyOnWriteMap采用的就是CopyOnWrite思想。

25. es实现
    1. 基本结构
       1. Cluster 包含多个Node的集群
       
       2. Node 集群服务单元
       
       3. Index 一个ES索引包含一个或多个物理分片，它只是这些分片的逻辑命名空间
       
       4. Type 一个index的不同分类，6.x后只能配置一个type，以后将移除
       
       5. Document 最基础的可被索引的数据单元，如一个JSON串
       
       6. Shards 一个分片是一个底层的工作单元，它仅保存全部数据中的一部分，它是一个Lucence实例 (一个lucene索引最大包含2,147,483,519 (= Integer.MAX_VALUE - 128)个文档数量)
       
       7. Replicas 分片备份，用于保障数据安全与分担检索压力    
       
       ES依赖一个重要的组件Lucene，关于数据结构的优化通常来说是对Lucene的优化，它是集群的一个存储于检索工作单元
       
       在Lucene中，分为索引(录入)与检索(查询)两部分，索引部分包含 分词器、过滤器、字符映射器 等，检索部分包含 查询解析器 等。
       
       一个Lucene索引包含多个segments，一个segment包含多个文档，每个文档包含多个字段，每个字段经过分词后形成一个或多个term。
    2. Lucene索引实现
       Lucene 索引文件结构主要的分为：词典、倒排表、正向文件、DocValues等
       
       倒排索引解决从词快速检索到相应文档ID, 但如果需要对结果进行排序、分组、聚合等操作的时候则需要根据文档ID快速找到对应的值。
       
       通过倒排索引代价却很高：需迭代索引里的每个词项并收集文档的列里面 token。这很慢而且难以扩展：随着词项和文档的数量增加，执行时间也会增加
    3. 关于ES索引与检索分片
       ES中一个索引由一个或多个lucene索引构成，一个lucene索引由一个或多个segment构成，其中segment是最小的检索域。
       
       数据具体被存储到哪个分片上： shard = hash(routing) % number_of_primary_shards
       
       默认情况下 routing参数是文档ID (murmurhash3),可通过 URL中的 _routing 参数指定数据分布在同一个分片中，index和search的时候都需要一致才能找到数据
       
       如果能明确根据_routing进行数据分区，则可减少分片的检索工作，以提高性能。      