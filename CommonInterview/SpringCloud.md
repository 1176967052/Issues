 1. 服务注册与发现Eureka使用
    启动一个服务注册中心，只需要一个注解@EnableEurekaServer，这个注解需要在springboot工程的启动application类上加
    eureka是一个高可用的组件，它没有后端缓存，每一个实例注册之后需要向注册中心发送心跳（因此可以在内存中完成），在默认情况下erureka server也是一个eureka client ,必须要指定一个 server
    当client向server注册时，它会提供一些元数据，例如主机和端口，URL，主页等。Eureka server 从每个client实例接收心跳消息。 如果心跳超时，则通常将该实例从注册server中删除。
 
 2. 服务消费者
    
    Spring cloud有两种服务调用方式，一种是ribbon+restTemplate，另一种是feign
    
    ribbon是一个负载均衡客户端，可以很好的控制htt和tcp的一些行为。Feign默认集成了ribbon。
    首先Ribbon会从 Eureka Client里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口号。
    
    简单轮询负载均衡（RoundRobin）： 以轮询的方式依次将请求调度不同的服务器，即每次调度执行i = (i + 1) mod n，并选出第i台服务器。
    
    随机负载均衡 （Random）： 随机选择状态为UP的Server
    
    加权响应时间负载均衡 （WeightedResponseTime）： 根据相应时间分配一个weight，相应时间越长，weight越小，被选中的可能性越低。
    
    区域感知轮询负载均衡（ZoneAvoidanceRule） ：复合判断server所在区域的性能和server的可用性选择server
    
    然后Ribbon就可以使用默认的Round Robin算法，从中选择一台机器
    
    Feign就会针对这台机器，构造并发起请求。
    
    @LoadBalanced注解表明这个restRemplate开启负载均衡的功能。
    通过@EnableDiscoveryClient向服务中心注册
   
    rest+ribbon 
    
    Feign默认集成了Ribbon，并和Eureka结合，默认实现了负载均衡的效果。Feign的一个关键机制就是使用了动态代理
    采用的是基于接口的注解。
    Feign 整合了ribbon，具有负载均衡的能力
    整合了Hystrix，具有熔断的能力
    
    首先，如果你对某个接口定义了@FeignClient注解，Feign就会针对这个接口创建一个动态代理
    
    接着你要是调用那个接口，本质就是会调用 Feign创建的动态代理，这是核心中的核心
    
    Feign的动态代理会根据你在接口上的@RequestMapping等注解，来动态构造出你要请求的服务的地址
    
    最后针对这个地址，发起请求、解析响应
    
    @EnableHystrix注解开启Hystrix
    Hystrix会搞很多个小小的线程池，比如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池。每个线程池里的线程就仅仅用于请求那个服务。
    
3. Zuul的主要功能是路由转发和过滤器。路由功能是微服务的一部分，比如／api/user转发到到user服务，/api/shop转发到到shop服务。zuul默认和Ribbon结合实现了负载均衡的功能。      

   在入口applicaton类加上注解@EnableZuulProxy，开启zuul的功能
   
   zuul不仅只是路由，并且还能过滤，做一些安全验证。ZuulFilter
   filterType：返回一个字符串代表过滤器的类型，在zuul中定义了四种不同生命周期的过滤器类型，具体如下：
   pre：路由之前
   routing：路由之时
   post： 路由之后
   error：发送错误调用
   filterOrder：过滤的顺序
   shouldFilter：这里可以写逻辑判断，是否要过滤，本文true,永远过滤。
   run：过滤器的具体逻辑。可用很复杂，包括查sql，nosql去判断该请求到底有没有权限访问。

4. 服务链路追踪组件zipkin


最后再来总结一下，上述几个Spring Cloud核心组件，在微服务架构中，分别扮演的角色：

 
5. 总结
   Eureka：各个服务启动时，Eureka Client都会将服务注册到Eureka Server，并且Eureka Client还可以反过来从Eureka Server拉取注册表，从而知道其他服务在哪里

   Ribbon：服务间发起请求的时候，基于Ribbon做负载均衡，从一个服务的多台机器中选择一台

   Feign：基于Feign的动态代理机制，根据注解和选择的机器，拼接请求URL地址，发起请求

   Hystrix：发起请求是通过Hystrix的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题

   Zuul：如果前端、移动端要调用后端系统，统一从Zuul网关进入，由Zuul网关转发请求给对应的服务
   
6. 熔断
   假设微服务A调用微服务B和微服务C，微服务B和微服务C又调用其它的微服务，这就是所谓的“扇出”。如果扇出的链路上某个微服务的调用响应时间过长或者不可用，对微服务A的调用就会占用越来越多的系统资源，进而引起系统崩溃，所谓的“雪崩效应”。
   熔断机制是应对雪崩效应的一种微服务链路保护机制。当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回错误的响应信息。当检测到该节点微服务调用响应正常后，恢复调用链路。
   
   在Spring Cloud框架里，熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败，就会启动熔断机制。
   在dubbo中也可利用nio超时+失败次数做熔断   
   
   Hystrix提供的熔断器具有自我反馈，自我恢复的功能，Hystrix会根据调用接口的情况，让熔断器在closed,open,half-open三种状态之间自动切换。
   
   1. open状态说明打开熔断，也就是服务调用方执行本地降级策略，不进行远程调用。
   2. closed状态说明关闭了熔断，这时候服务调用方直接发起远程调用。
   3. half-open状态，则是一个中间状态，当熔断器处于这种状态时候，直接发起远程调用。
   
   状态转化：
   1. closed->open:正常情况下熔断器为closed状态，当访问同一个接口次数超过设定阈值并且错误比例超过设置错误阈值时候，就会打开熔断机制，这时候熔断器状态从closed->open。
   
   2. open->half-open:当服务接口对应的熔断器状态为open状态时候，所有服务调用方调用该服务方法时候都是执行本地降级方法，那么什么时候才会恢复到远程调用那？Hystrix提供了一种测试策略，也就是设置了一个时间窗口，从熔断器状态变为open状态开始的一个时间窗口内，调用该服务接口时候都委托服务降级方法进行执行。如果时间超过了时间窗口，则把熔断状态从open->half-open,这时候服务调用方调用服务接口时候，就可以发起远程调用而不再使用本地降级接口，如果发起远程调用还是失败，则重新设置熔断器状态为open状态，从新记录时间窗口开始时间。
      滑动窗口数据结构来记录当前时间窗的各种事件(成功,失败,超时,线程池拒绝等)的计数. 
      		事件产生时, 数据结构根据当前时间确定使用旧桶还是创建新桶来计数, 并在桶中对计数器经行修改. 
      		这些修改是多线程并发执行的, 代码中有不少加锁操作,逻辑较为复杂.
      1.5之后的滑动窗口实现
      		Hystrix在这些版本中开始使用RxJava的Observable.window()实现滑动窗口.
      		RxJava的window使用后台线程创建新桶, 避免了并发创建桶的问题.
      		同时RxJava的单线程无锁特性也保证了计数变更时的线程安全. 从而使代码更加简洁. 		
      
   3. half-open->closed: 当熔断器状态为half-open,这时候服务调用方调用服务接口时候，就可以发起远程调用而不再使用本地降级接口，如果发起远程调用成功，则重新设置熔断器状态为closed状态。
   
   缓存雪崩的应对策略
   1. 流量控制
   	  网关限流:因为Nginx的高性能, 目前一线互联网公司大量采用Nginx+Lua的网关进行流量控制, 由此而来的OpenResty也越
   			 来越热门.
   	  用户交互限流：1. 采用加载动画,提高用户的忍耐等待时间. 2. 提交按钮添加强制等待时间机制.
   	  关闭重试
   2. 改进缓存模式：
      缓存预加载
   	  同步改为异步刷新
   3. 服务器自动扩容
   	  AWS的auto scaling
   4. 服务调用者降级服务
   	  资源隔离:资源隔离主要是对调用服务的线程池进行隔离.
   	  对依赖服务进行分类：我们根据具体业务,将依赖服务分为: 强依赖和若依赖. 强依赖服务不可用会导致当前业务中止,而弱	
   						依赖服务的不可用不会导致当前业务的中止.
   	  不可用服务的调用快速失败:不可用服务的调用快速失败一般通过 超时机制, 熔断器 和熔断后的 降级方法 来实现.

   首先讲一个Hystrix的设计原则
   1. 资源隔离，通过将每个依赖服务分配独立的线程池进行资源隔离, 从而避免服务雪崩. 
   2. 熔断器，熔断器模式定义了熔断器开关相互转换的逻辑
   3. 命令模式，Hystrix使用命令模式(继承HystrixCommand类或者是HystrixObservableCommand类)来包裹具体的服务调用逻辑(run方法), 并在命令模式中添加了服务调用失败后的降级逻辑(getFallback).
      同时我们在Command的构造方法中可以定义当前服务线程池和熔断器的相关参数
   
   Hystrix内部处理逻辑：   
   1. 构建Hystrix的Command对象, 调用执行方法.
   2. Hystrix检查当前服务的熔断器开关是否开启, 若开启, 则执行降级服务getFallback方法.
   3. 若熔断器开关关闭, 则Hystrix检查当前服务的线程池是否能接收新的请求（线程池拒绝策略）, 若超过线程池已满, 则执行降级服务getFallback方法.
   4. 若线程池接受请求, 则Hystrix开始执行服务调用具体逻辑run方法.
   5. 若服务执行失败, 则执行降级服务getFallback方法, 并将执行结果上报Metrics更新服务健康状况.
   6. 若服务执行超时, 则执行降级服务getFallback方法, 并将执行结果上报Metrics更新服务健康状况.
   7. 若服务执行成功, 返回正常结果.
   8. 若服务降级方法getFallback执行成功, 则返回降级结果.
   9. 若服务降级方法getFallback执行失败, 则抛出异常.   
   
   
   熔段解决如下几个问题：
   1. 当所依赖的对象不稳定时，能够起到快速失败的目的
   2. 快速失败后，能够根据一定的算法动态试探所依赖对象是否恢复
7. 降级
   降级是指自己的待遇下降了，从RPC调用环节来讲，就是去访问一个本地的伪装者而不是真实的服务。
   
   降级和熔断的区别和相同点
   相同点：   
   1. 目的很一致，都是从可用性可靠性着想，为防止系统的整体缓慢甚至崩溃，采用的技术手段；
   2. 最终表现类似，对于两者来说，最终让用户体验到的是某些功能暂时不可达或不可用；
   3. 粒度一般都是服务级别，当然，业界也有不少更细粒度的做法，比如做到数据持久层（允许查询，不允许增删改）；
   4. 自治性要求很高，熔断模式一般都是服务基于策略的自动触发，降级虽说可人工干预，但在微服务架构下，完全靠人显然不可能，开关预置、配置中心都是必要手段；
 
   区别：
   1. 触发原因不太一样，服务熔断一般是某个服务（下游服务）故障引起，而服务降级一般是从整体负荷考虑；
   2. 管理目标的层次不太一样，熔断其实是一个框架级的处理，每个微服务都需要（无层级之分），而降级一般需要对业务有层级之分（比如降级一般是从最外围服务开始）
   3. 实现方式不太一样；服务降级具有代码侵入性(由控制器完成/或自动降级)，熔断一般称为自我熔断。

8. 限流
   限流的目的是通过对并发访问/请求进行限速或者一个时间窗口内的的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务（定向到错误页或告知资源没有了）、排队或等待（比如秒杀、评论、下单）、降级（返回兜底数据或默认数据，如商品详情页库存默认有货）。
   
   一般开发高并发系统常见的限流有：限制总并发数（比如数据库连接池、线程池）、限制瞬时并发数（如nginx的limit_conn模块，用来限制瞬时并发连接数）、限制时间窗口内的平均速率（如Guava的RateLimiter、nginx的limit_req模块，限制每秒的平均速率）；其他还有如限制远程接口调用速率、限制MQ的消费速率。另外还可以根据网络连接数、网络流量、CPU或内存负载等来限流。
   
   限流算法：
   1. 漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率
   2. 令牌桶算法(Token Bucket)和 Leaky Bucket 效果一样但方向相反的算法,更加容易理解.随着时间流逝,系统会按恒定1/QPS时间间隔(如果QPS=100,则间隔是10ms)往桶里加入Token(想象和漏洞漏水相反,有个水龙头在不断的加水),如果桶已经满了就不再加了.新请求来临时,会各自拿走一个Token,如果没有Token可拿了就阻塞或者拒绝服务.
 
   基于Redis功能的实现限流:
   假设一个用户（用IP判断）每分钟访问某一个服务接口的次数不能超过10次，那么我们可以在Redis中创建一个键，并此时我们就设置键的过期时间为60秒，每一个用户对此服务接口的访问就把键值加1，在60秒内当键值增加到10的时候，就禁止访问服务接口。

9. zipkin分布式的跟踪系统
   结构：
   1. Collector 接受或者收集各个应用传输的数据
   2. Storage：负责存储接收到的数据，默认是存储在内存当中的，也可以支持存在MySQL当中
   3. API：负责查询Storage中存储的数据，主要是提供给Web UI来使用
   4. Web：主要是提供简单的web界面
   
   工作流程
   一个应用的代码发起HTTP get请求，经过Trace框架拦截，然后
   
   把当前调用链的Trace信息添加到HTTP Header里面
   记录当前调用的时间戳
   发送HTTP请求，把trace相关的header信息携带上
   调用结束之后，记录当前调用花费的时间
   然后把上面流程产生的 信息汇集成一个span，把这个span信息上传到zipkin的Collector模块

10. 注册中心相关
    各个服务内的Eureka Client组件，默认情况下，每隔30秒会发送一个请求到Eureka Server，来拉取最近有变化的服务信息
    Eureka还有一个心跳机制，各个Eureka Client每隔30秒会发送一次心跳到Eureka Server，通知人家说，哥们，我这个服务实例还活着
    
    Eureka Server到底是用什么来存储注册表的？名字叫做registry的CocurrentHashMap，就是注册表的核心结构。
    Eureka Server的注册表直接基于纯内存，即在内存里维护了一个数据结构。
    各个服务的注册、服务下线、服务故障，全部会在内存里维护和更新这个注册表。
    这个ConcurrentHashMap的key就是服务名称，比如“inventory-service”，就是一个服务名称。value则代表了一个服务的多个服务实例。
    Map<String, Lease<InstanceInfo>>这个InstanceInfo就代表了服务实例的具体信息，比如机器的ip地址、hostname以及端口号。而这个Lease，里面则会维护每个服务最近一次发送心跳的时间
    
    Eureka Server为了避免同时读写内存数据结构造成的并发冲突问题，还采用了多级缓存机制来进一步提升服务请求的响应速度。
    在拉取注册表的时候：
    首先从ReadOnlyCacheMap里查缓存的注册表。
    
    若没有，就找ReadWriteCacheMap里缓存的注册表。
    
    如果还没有，就从内存中获取实际的注册表数据。
    
    在注册表发生变更的时候：
    会在内存中更新变更的注册表数据，同时过期掉ReadWriteCacheMap。
    
    此过程不会影响ReadOnlyCacheMap提供人家查询注册表。
    
    一段时间内（默认30秒），各服务拉取注册表会直接读ReadOnlyCacheMap
    
    30秒过后，Eureka Server的后台线程发现ReadWriteCacheMap已经清空了，也会清空ReadOnlyCacheMap中的缓存
    
    下次有服务拉取注册表，又会从内存中获取最新的数据了，同时填充各个缓存。
    
    多级缓存机制的优点是什么？
    
    尽可能保证了内存注册表数据不会出现频繁的读写冲突问题。
    
    并且进一步保证对Eureka Server的大量请求，都是快速从纯内存走，性能极高。
 11. 优化总结
     假设你的服务A，每秒钟会接收30个请求，同时会向服务B发起30个请求，然后每个请求的响应时长经验值大概在200ms，那么你的hystrix线程池需要多少个线程呢？
     
     计算公式是：30（每秒请求数量） * 0.2（每个请求的处理秒数） + 4（给点缓冲buffer） = 10（线程数量）。
     
     如果对上述公式存在疑问，不妨反过来推算一下，为什么10个线程可以轻松抗住每秒30个请求？
     
     一个线程200毫秒可以执行完一个请求，那么一个线程1秒可以执行5个请求，理论上，只要6个线程，每秒就可以执行30个请求。
     
     也就是说，线程里的10个线程中，就6个线程足以抗住每秒30个请求了。剩下4个线程都在玩儿，空闲着。那为啥要多搞4个线程呢？很简单，因为你要留一点buffer空间。
     
     线程数量OK了，那么请求的超时时间设置为多少？答案是300毫秒。
