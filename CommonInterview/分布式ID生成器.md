#### 
1. 使用原因
   
   在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。

2. 业务系统对ID号的要求有哪些呢？
   
   1. 全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。
   
   2. 趋势递增：在MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能。
   
   3. 单调递增：保证下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求。
   
   4. 信息安全：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则。
   
   上述123对应三类不同的场景，3和4需求还是互斥的，无法使用同一个方案满足。
   
   由此总结下一个ID生成系统应该做到如下几点：
   
   1. 平均延迟和TP999延迟都要尽可能低；
   
   2. 可用性5个9；
   
   3. 高QPS。 

3. 常见生成方案介绍
   
   1. UUID
      
      UUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：550e8400-e29b-41d4-a716-446655440000，到目前为止业界一共有5种方式生成UUID 
      
      优点：性能非常高：本地生成，没有网络消耗。 
      
      缺点： 
      1. 不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。
         
      2. 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。
         
      3. ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用：
         
         ① MySQL官方有明确的建议主键要尽量越短越好，36个字符长度的UUID不符合要求。
         
         ② 对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能。
   
   2. 类snowflake方案
      
      这种方案大致来说是一种以划分命名空间（UUID也算，由于比较常见，所以单独分析）来生成ID的一种算法，这种方案把64-bit分别划分成多段，分开来标示机器、时间等。
      
      41-bit的时间可以表示（1L<<41）/(1000L*3600*24*365)=69年的时间，10-bit机器可以分别表示1024台机器。如果我们对IDC划分有需求，还可以将10-bit分5-bit给IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，可以根据自身需求定义。12个自增序列号可以表示2^12个ID，理论上snowflake方案的QPS约为409.6w/s，这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的。
      
      优点：
      1. 毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。
      
      2. 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。
      
      3. 可以根据自身业务特性分配bit位，非常灵活。
      
      缺点：强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。
      
   3. 应用举例Mongdb objectID   
      
      MongoDB官方文档 ObjectID可以算作是和snowflake类似方法，通过“时间+机器码+pid+inc”共12个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。
    
   4. 数据库生成
      
      优点：
      1. 非常简单，利用现有数据库系统的功能实现，成本小，有DBA专业维护。
      2. ID号单调自增，可以实现一些对ID有特殊要求的业务。
      
      缺点： 
      1. 强依赖DB，当DB异常时整个系统不可用，属于致命问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。
      2. ID发号性能瓶颈限制在单台MySQL的读写性能。
      
      对于MySQL性能问题，可用如下方案解决：在分布式系统中我们可以多部署几台机器，每台机器设置不同的初始值，且步长和机器数相等。比如有两台机器。设置步长step为2，TicketServer1的初始值为1（1，3，5，7，9，11...）、TicketServer2的初始值为2（2，4，6，8，10...）。
      
      TicketServer1:
      
      auto-increment-increment = 2
      
      auto-increment-offset = 1
      
      TicketServer2:
      
      auto-increment-increment = 2
      
      auto-increment-offset = 2
      
      这种架构貌似能够满足性能的需求，但有以下几个缺点：
      
      1. 系统水平扩展比较困难，比如定义好了步长和机器台数之后，如果要添加机器该怎么做？假设现在只有一台机器发号是1,2,3,4,5（步长是1），这个时候需要扩容机器一台。可以这样做：把第二台机器的初始值设置得比第一台超过很多，比如14（假设在扩容时间之内第一台不可能发到14），同时设置步长为2，那么这台机器下发的号码都是14以后的偶数。然后摘掉第一台，把ID值保留为奇数，比如7，然后修改第一台的步长为2。让它符合我们定义的号段标准，对于这个例子来说就是让第一台以后只能产生奇数。扩容方案看起来复杂吗？貌似还好，现在想象一下如果我们线上有100台机器，这个时候要扩容该怎么做？简直是噩梦。所以系统水平扩展方案复杂难以实现。
      
      2. ID没有了单调递增的特性，只能趋势递增，这个缺点对于一般业务需求不是很重要，可以容忍。
      
      3. 数据库压力还是很大，每次获取ID都得读写一次数据库，只能靠堆机器来提高性能。
      
4. 美团的Leaf方案
   
   1. Leaf-segment数据库方案
      
      在使用数据库的方案上，做了如下改变： 
      
      1. 原方案每次获取ID都得读写一次数据库，造成数据库压力大。改为利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。
      
      2. 各个业务不同的发号需求用biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响。如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对biz_tag分库分表就行。
      
      重要字段说明：biz_tag用来区分业务，max_id表示该biz_tag目前所被分配的ID号段的最大值，step表示每次分配的号段长度。原来获取ID每次都需要写数据库，现在只需要把step设置得足够大，比如1000。那么只有当1000个号被消耗完了之后才会去重新读写一次数据库。读写数据库的频率从1减小到了1/step
      
      Begin UPDATE table SET max_id=max_id+step WHERE biz_tag = xxx SELECT tag, max_id, step FROM table WHERE biz_tag=xxx Commit

      优点：
      
      1. Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景。
      
      2. ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求。
      
      3. 容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务。
      
      4. 可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来。
      
      缺点：
      
      1. ID号码不够随机，能够泄露发号数量的信息，不太安全。
      
      2. TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺。
      
      3. DB宕机会造成整个系统不可用。
       
      优化：双buffer机制
      采用双buffer的方式，Leaf服务内部有两个号段缓存区segment。当前号段已下发10%时，如果下一个号段未更新，则另启一个更新线程去更新下一个号段。当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复。
   
   2. Leaf-snowflake方案
      完全沿用snowflake方案的bit位设计，即是“1+41+10+12”的方式组装ID号。
      
      1. 启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）。
      2. 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务。
      3. 如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务。 
      
      解决时钟回拨问题：
      1. 若写过，则用自身系统时间与leaf_forever/${self}节点记录时间做比较，若小于leaf_forever/${self}时间则认为机器时间发生了大步长回拨，服务启动失败并报警。
      2. 若未写过，证明是新服务节点，直接创建持久节点leaf_forever/${self}并写入自身系统时间，接下来综合对比其余Leaf节点的系统时间来判断自身系统时间是否准确，具体做法是取leaf_temporary下的所有临时节点(所有运行中的Leaf-snowflake节点)的服务IP：Port，然后通过RPC请求得到所有节点的系统时间，计算sum(time)/nodeSize。
      3. 若abs( 系统时间-sum(time)/nodeSize ) < 阈值，认为当前系统时间准确，正常启动服务，同时写临时节点leaf_temporary/${self} 维持租约。
      4. 否则认为本机系统时间发生大步长偏移，启动失败并报警。
      5. 每隔一段时间(3s)上报自身系统时间写入leaf_forever/${self}。   

